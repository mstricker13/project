batch size set to 32
changed architecture to 1 layer per encoder/decoder. 
---------------------------------------------------------
Decreased units size from 512 to 32
decreased dropout from 0.5 to 0.3
added activation function in last dense layer as softmax
learning rate + batch norm